{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math, time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn import datasets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X, y, seed=None):\n",
    "    \"\"\" Random shuffle of the samples in X and y \"\"\"\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    idx = np.arange(X.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "def train_test_split(X, y, test_size=0.5, shuffle=True, seed=None):\n",
    "    \"\"\" Split the data into train and test sets \"\"\"\n",
    "    if shuffle:\n",
    "        X, y = shuffle_data(X, y, seed)\n",
    "    # Split the training data from test data in the ratio specified in\n",
    "    # test_size\n",
    "    split_i = len(y) - int(len(y) // (1 / test_size))\n",
    "    X_train, X_test = X[:split_i], X[split_i:]\n",
    "    y_train, y_test = y[:split_i], y[split_i:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    \"\"\" Calculates the l2 distance between two vectors \"\"\"\n",
    "    distance = 0\n",
    "    # Squared distance between each coordinate\n",
    "    for i in range(len(x1)):\n",
    "        distance += pow((x1[i] - x2[i]), 2)\n",
    "    return math.sqrt(distance)\n",
    "\n",
    "def normalize(X, axis=-1, order=2):\n",
    "    \"\"\" Normalize the dataset X \"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(X, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return X / np.expand_dims(l2, axis)\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\" Compare y_true to y_pred and return the accuracy \"\"\"\n",
    "    accuracy= np.sum(y_true == y_pred, axis=0) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "def to_categorical(x, n_col=None):\n",
    "    \"\"\" One-hot encoding of nominal values \"\"\"\n",
    "    if not n_col:\n",
    "        n_col = np.amax(x) + 1\n",
    "    one_hot = np.zeros((x.shape[0], n_col))\n",
    "    one_hot[np.arange(x.shape[0]), x] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# class KNN():\n",
    "#     \"\"\" K Nearest Neighbors classifier.\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     k: int\n",
    "#         The number of closest neighbors that will determine the class of the \n",
    "#         sample that we wish to predict.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, k=5):\n",
    "#         self.k = k\n",
    "\n",
    "#     def _vote(self, neighbor_labels):\n",
    "#         \"\"\" Return the most common class among the neighbor samples \"\"\"\n",
    "#         counts = np.bincount(neighbor_labels.astype('int'))\n",
    "#         return counts.argmax()\n",
    "\n",
    "#     def predict(self, X_test, X_train, y_train):\n",
    "#         y_pred = np.empty(X_test.shape[0])\n",
    "#         # Determine the class of each sample\n",
    "#         for i, test_sample in enumerate(X_test):\n",
    "#             # Sort the training samples by their distance to the test sample and get the K nearest\n",
    "#             idx = np.argsort([euclidean_distance(test_sample, x) for x in X_train])[:self.k]\n",
    "#             # Extract the labels of the K nearest neighboring training samples\n",
    "#             k_nearest_neighbors = np.array([y_train[i] for i in idx])\n",
    "#             # Label sample as the most common class label\n",
    "#             count= self._vote(k_nearest_neighbors)\n",
    "#             y_pred[i] = count\n",
    "\n",
    "#         return y_pred\n",
    "\n",
    "# data = datasets.load_iris()\n",
    "# X = data.data\n",
    "# y = data.target\n",
    "# X_train, X_test, y_train, y_test = train_test_split(normalize(X), y, test_size=0.2)\n",
    "\n",
    "# clf = KNN(k=5)\n",
    "# y_pred = clf.predict(X_test, X_train, y_train)\n",
    "# accuracy_his = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# from supervised_learning.knn import KNN\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and 1 > 0) else \"cpu\")\n",
    "# X_train, X_test, y_train, y_test= Variable(torch.from_numpy(X_train)), Variable(torch.from_numpy(X_test)), Variable(torch.from_numpy(y_train)), Variable(torch.from_numpy(y_test))\n",
    "\n",
    "# classifier= KNN(k= 5)#.to(device)\n",
    "# y_pred_mine = classifier.predict(X_test, X_train, y_train)\n",
    "\n",
    "# def accuracy_score(y_true, y_pred):\n",
    "#     \"\"\" Compare y_true to y_pred and return the accuracy \"\"\"\n",
    "#     accuracy = torch.sum(y_true == y_pred, axis=0).item() / len(y_true)\n",
    "#     return accuracy\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred_mine)\n",
    "# print (\"Accuracy his:\", accuracy_his)\n",
    "# print (\"Accuracy mine:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from mlfromscratch.deep_learning.activation_functions import Sigmoid\n",
    "\n",
    "# class LogisticRegression():\n",
    "#     \"\"\" Logistic Regression classifier.\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     learning_rate: float\n",
    "#         The step length that will be taken when following the negative gradient during\n",
    "#         training.\n",
    "#     gradient_descent: boolean\n",
    "#         True or false depending if gradient descent should be used when training. If\n",
    "#         false then we use batch optimization by least squares.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, learning_rate=.1, gradient_descent=True):\n",
    "#         self.param = None\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.gradient_descent = gradient_descent\n",
    "#         self.sigmoid = Sigmoid()\n",
    "\n",
    "#     def _initialize_parameters(self, X):\n",
    "#         n_features = np.shape(X)[1]\n",
    "#         # Initialize parameters between [-1/sqrt(N), 1/sqrt(N)]\n",
    "#         limit = 1 / math.sqrt(n_features)\n",
    "#         self.param = np.random.uniform(-limit, limit, (n_features,))\n",
    "\n",
    "#     def fit(self, X, y, n_iterations=4000):\n",
    "#         self._initialize_parameters(X)\n",
    "#         # Tune parameters for n iterations\n",
    "#         for i in range(n_iterations):\n",
    "#             # Make a new prediction\n",
    "#             y_pred = self.sigmoid(X.dot(self.param))\n",
    "#             if self.gradient_descent:\n",
    "#                 # Move against the gradient of the loss function with\n",
    "#                 # respect to the parameters to minimize the loss\n",
    "#                 diff= -(y -y_pred)\n",
    "#                 self.param -= self.learning_rate * diff.dot(X)\n",
    "#             else:\n",
    "#                 # Make a diagonal matrix of the sigmoid gradient column vector\n",
    "#                 diag_gradient = make_diagonal(self.sigmoid.gradient(X.dot(self.param)))\n",
    "#                 # Batch opt:\n",
    "#                 self.param = np.linalg.pinv(X.T.dot(diag_gradient).dot(X)).dot(X.T).dot(diag_gradient.dot(X).dot(self.param) + y - y_pred)\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         y_pred = np.round(self.sigmoid(X.dot(self.param))).astype(int)\n",
    "#         return y_pred\n",
    "\n",
    "# data = datasets.load_iris()\n",
    "# X = normalize(data.data[data.target != 0])\n",
    "# y = data.target[data.target != 0]\n",
    "# y[y == 1] = 0\n",
    "# y[y == 2] = 1\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, seed=1)\n",
    "\n",
    "# clf = LogisticRegression(gradient_descent=True)\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print (\"Accuracy:\", accuracy)\n",
    "\n",
    "# from supervised_learning.logisticregression import LogisticRegression\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and 1 > 0) else \"cpu\")\n",
    "# X_train, X_test, y_train, y_test= Variable(torch.from_numpy(X_train)), Variable(torch.from_numpy(X_test)), Variable(torch.from_numpy(y_train)), Variable(torch.from_numpy(y_test))\n",
    "# clf= LogisticRegression(gradient_descent= True).to(device)\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred= clf.predict(X_test)\n",
    "\n",
    "# def accuracy_score(y_true, y_pred):\n",
    "#     \"\"\" Compare y_true to y_pred and return the accuracy \"\"\"\n",
    "#     accuracy = torch.sum(y_true == y_pred, axis=0).item() / len(y_true)\n",
    "#     return accuracy\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print (\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.41379310344827586\n",
      "Accuracy: 0.4482758620689655\n"
     ]
    }
   ],
   "source": [
    "from mlfromscratch.deep_learning.activation_functions import Sigmoid, Softmax\n",
    "from mlfromscratch.deep_learning.loss_functions import CrossEntropy\n",
    "\n",
    "class MultilayerPerceptron():\n",
    "    \"\"\"Multilayer Perceptron classifier. A fully-connected neural network with one hidden layer.\n",
    "    Unrolled to display the whole forward and backward pass.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_hidden: int:\n",
    "        The number of processing nodes (neurons) in the hidden layer. \n",
    "    n_iterations: float\n",
    "        The number of training iterations the algorithm will tune the weights for.\n",
    "    learning_rate: float\n",
    "        The step length that will be used when updating the weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_hidden, n_iterations=3000, learning_rate=0.01):\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_iterations = n_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_activation = Sigmoid()\n",
    "        self.output_activation = Softmax()\n",
    "        self.loss = CrossEntropy()\n",
    "\n",
    "    def _initialize_weights(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        _, n_outputs = y.shape\n",
    "        # Hidden layer\n",
    "        limit   = 1 / math.sqrt(n_features)\n",
    "#         print((n_features, self.n_hidden))\n",
    "        self.W  = np.random.uniform(-limit, limit, (n_features, self.n_hidden))\n",
    "        self.w0 = np.zeros((1, self.n_hidden))\n",
    "        # Output layer\n",
    "        limit   = 1 / math.sqrt(self.n_hidden)\n",
    "#         print((self.n_hidden, n_outputs))\n",
    "        self.V  = np.random.uniform(-limit, limit, (self.n_hidden, n_outputs))\n",
    "        self.v0 = np.zeros((1, n_outputs))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self._initialize_weights(X, y)\n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "\n",
    "            # ..............\n",
    "            #  Forward Pass\n",
    "            # ..............\n",
    "\n",
    "            # HIDDEN LAYER\n",
    "            hidden_input = X.dot(self.W) + self.w0\n",
    "            hidden_output = self.hidden_activation(hidden_input)\n",
    "            # OUTPUT LAYER\n",
    "#             print('Output Layer: ', hidden_output.shape, self.V.shape)\n",
    "            output_layer_input = hidden_output.dot(self.V) + self.v0\n",
    "            y_pred = self.output_activation(output_layer_input)\n",
    "\n",
    "            # ...............\n",
    "            #  Backward Pass\n",
    "            # ...............\n",
    "\n",
    "            # OUTPUT LAYER\n",
    "            # Grad. w.r.t input of output layer\n",
    "            grad_wrt_out_l_input = self.loss.gradient(y, y_pred) * self.output_activation.gradient(output_layer_input)\n",
    "            grad_v = hidden_output.T.dot(grad_wrt_out_l_input)\n",
    "            grad_v0 = np.sum(grad_wrt_out_l_input, axis=0, keepdims=True)\n",
    "            # HIDDEN LAYER\n",
    "            # Grad. w.r.t input of hidden layer\n",
    "            grad_wrt_hidden_l_input = grad_wrt_out_l_input.dot(self.V.T) * self.hidden_activation.gradient(hidden_input)\n",
    "            grad_w = X.T.dot(grad_wrt_hidden_l_input)\n",
    "            grad_w0 = np.sum(grad_wrt_hidden_l_input, axis=0, keepdims=True)\n",
    "            \n",
    "#             print(grad_v.shape, self.V.shape)\n",
    "#             print(grad_v0.shape, self.v0.shape)\n",
    "#             print(grad_w.shape, self.W.shape)\n",
    "#             print(grad_w0.shape, self.w0.shape)\n",
    "            \n",
    "            # Update weights (by gradient descent)\n",
    "            # Move against the gradient to minimize loss\n",
    "            self.V  -= self.learning_rate * grad_v\n",
    "            self.v0 -= self.learning_rate * grad_v0\n",
    "            self.W  -= self.learning_rate * grad_w\n",
    "            self.w0 -= self.learning_rate * grad_w0\n",
    "\n",
    "    # Use the trained model to predict labels of X\n",
    "    def predict(self, X):\n",
    "        # Forward pass:\n",
    "        hidden_input = X.dot(self.W) + self.w0\n",
    "        hidden_output = self.hidden_activation(hidden_input)\n",
    "        output_layer_input = hidden_output.dot(self.V) + self.v0\n",
    "        y_pred = self.output_activation(output_layer_input)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "data = datasets.load_iris()\n",
    "X = normalize(data.data[data.target != 0])\n",
    "y = data.target[data.target != 0]\n",
    "y = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, seed=1)\n",
    "clf= MultilayerPerceptron(n_hidden= 20, n_iterations= 10000, learning_rate= 1e-4)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = np.argmax(clf.predict(X_test), axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print (\"Accuracy:\", accuracy)\n",
    "\n",
    "from supervised_learning.multilayerperceptron import MultiLayerPerceptron\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and 1 > 0) else \"cpu\")\n",
    "X_train, X_test, y_train, y_test= Variable(torch.from_numpy(X_train)), Variable(torch.from_numpy(X_test)), Variable(torch.from_numpy(y_train)), Variable(torch.from_numpy(y_test))\n",
    "\n",
    "clf= MultiLayerPerceptron(n_hidden= 20, n_iterations= 10000, learning_rate= 1e-4).to(device)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = torch.argmax(clf.predict(X_test), dim=1)\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\" Compare y_true to y_pred and return the accuracy \"\"\"\n",
    "    accuracy = torch.sum(y_true == y_pred, axis=0).item() / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print (\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
